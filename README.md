# ğŸ“š Wikipedia RAG Chatbot

A Retrieval-Augmented Generation (RAG) chatbot that answers questions from any Wikipedia article using **LangChain**, **FAISS**, **HuggingFace Embeddings**, and **Groq LLM** â€” with a clean **Gradio** chat interface.

---

## ğŸ§  How It Works

```
Wikipedia Article
      â†“
   Chunking  (RecursiveCharacterTextSplitter)
      â†“
  Embeddings  (sentence-transformers/all-MiniLM-L6-v2)
      â†“
 FAISS VectorStore  (saved to disk)
      â†“
  User Question
      â†“
   Router  â†’ too vague? ask to clarify
      â†“
  Retrieval  â†’ top 3 relevant chunks
      â†“
  Groq LLM  (llama-3.1-8b-instant)
      â†“
  JSON Response  (answer + quotes + confidence)
      â†“
  Gradio UI
```

---

## ğŸš€ Quick Start (Google Colab)

1. Open [Google Colab](https://colab.research.google.com)
2. Open `wikipedia_rag_chatbot.py`
3. Copy each `CELL` block into a **separate Colab cell** in order
4. Add your **Groq API key** in Cell 2
5. Run all cells top to bottom
6. Enter any Wikipedia topic when prompted (e.g. `Diabetes`, `Black hole`)
7. Chat with the bot in the Gradio UI

> âš ï¸ Each `%%writefile` cell must be in its **own Colab cell** â€” do not merge them.

---

## ğŸ”‘ Get a Free Groq API Key

1. Go to [console.groq.com](https://console.groq.com)
2. Sign up for free
3. Create an API key
4. Paste it in Cell 2

---

## ğŸ—‚ï¸ Project Structure

```
wikipedia-rag-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ load_wikipedia.py   # Fetches article from Wikipedia API
â”‚   â”œâ”€â”€ chunking.py         # Splits text into chunks
â”‚   â”œâ”€â”€ embeddings.py       # Creates and saves FAISS vectorstore
â”‚   â”œâ”€â”€ retrieval.py        # Retrieves relevant chunks
â”‚   â”œâ”€â”€ prompts.py          # Builds the LLM prompt
â”‚   â”œâ”€â”€ router.py           # Routes vague vs clear questions
â”‚   â””â”€â”€ rag_pipeline.py     # Orchestrates the full pipeline
â”œâ”€â”€ data/
â”‚   â””â”€â”€ topic.txt           # Saved Wikipedia article
â””â”€â”€ vectorstore/            # Saved FAISS index (persists across restarts)
```

---

## âœ¨ Features

- **Any Wikipedia topic** â€” not hardcoded, user picks at runtime
- **Smart router** â€” detects vague questions and asks for clarification
- **Vectorstore saved to disk** â€” no rebuilding on Colab restart
- **Structured JSON responses** â€” answer, confidence score, supporting quotes
- **Source chunks displayed** â€” full transparency on what the LLM used
- **Gradio chat UI** â€” clean interface with example questions

---

## ğŸ› ï¸ Tech Stack

| Component | Library |
|-----------|---------|
| LLM | Groq (llama-3.1-8b-instant) |
| Embeddings | HuggingFace sentence-transformers |
| Vector DB | FAISS |
| Orchestration | LangChain |
| UI | Gradio |
| Data Source | Wikipedia API |

---

## ğŸ“¦ Dependencies

```
langchain
langchain-community
langchain-huggingface
langchain-text-splitters
sentence-transformers
faiss-cpu
groq
requests
gradio
```

---

## ğŸ“„ License

MIT License â€” free to use and modify.
